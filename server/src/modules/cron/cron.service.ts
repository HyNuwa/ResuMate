import * as cron from 'node-cron';
import { scraperService, ExtractJobResponse } from '../scraper';

/**
 * URLs de fuentes de ofertas de empleo para scraping automÃ¡tico
 * Puedes personalizar estas URLs segÃºn tus fuentes preferidas
 */
const JOB_SOURCES = [
  'https://www.ycombinator.com/jobs', // Fuente real rica en datos y links
  // 'https://weworkremotely.com/remote-programming-jobs', // Ejemplo alternativo
];

export class CronJobService {
  private jobs: Map<string, cron.ScheduledTask> = new Map();

  /**
   * Inicia todos los cron jobs
   */
  start() {
    console.log('ğŸ• Iniciando servicios de cron jobs...');
    
    // Job semanal: Scraping de ofertas de empleo
    // Corre cada domingo a las 2 AM
    this.scheduleWeeklyJobScraping();
    
    console.log('âœ… Cron jobs iniciados correctamente');
  }

  /**
   * Job semanal para scrapear ofertas de empleo
   * Corre cada domingo a las 2:00 AM
   */
  private scheduleWeeklyJobScraping() {
    const task = cron.schedule('0 2 * * 0', async () => {
      console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
      console.log('ğŸ”„ Iniciando scraping semanal de ofertas...');
      console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
      
      try {
        await this.scrapeJobPostings();
        console.log('\nâœ… Scraping semanal completado');
      } catch (error) {
        console.error('\nâŒ Error en scraping semanal:', error);
      }
    }, {
      timezone: 'America/Argentina/Buenos_Aires' // Ajustar segÃºn tu zona horaria
    });

    this.jobs.set('weekly_job_scraping', task);
    console.log('âœ… Job semanal configurado: Domingos 2:00 AM (scraping ofertas)');
  }

  /**
   * FunciÃ³n principal de scraping
   * Extrae ofertas de las fuentes configuradas
   */
  private async scrapeJobPostings(): Promise<void> {
    const allJobUrls: string[] = [];

    // Paso 1: Obtener URLs de ofertas de cada fuente
    console.log('ğŸ“‹ Extrayendo URLs de ofertas...');
    for (const source of JOB_SOURCES) {
      try {
        console.log(`  - Procesando: ${source}`);
        const urls = await scraperService.extractJobListings(source);
        allJobUrls.push(...urls);
        console.log(`    âœ“ Encontradas ${urls.length} ofertas`);
      } catch (error) {
        console.error(`    âœ— Error en ${source}:`, error);
      }
    }

    console.log(`\nğŸ“Š Total de ofertas encontradas: ${allJobUrls.length}`);

    if (allJobUrls.length === 0) {
      console.log('âš ï¸  No se encontraron ofertas para procesar');
      return;
    }

    // Paso 2: Extraer datos de cada oferta (en lotes)
    console.log('\nğŸ” Extrayendo datos de ofertas...');
    const results = await scraperService.extractMultipleJobs(allJobUrls, true, 3);

    // Paso 3: Procesar resultados
    const successful = results.filter(r => r.success);
    const failed = results.filter(r => !r.success);

    console.log(`\nâœ… Exitosas: ${successful.length}`);
    console.log(`âŒ Fallidas: ${failed.length}`);

    // Paso 4: Guardar ofertas en la base de datos (implementar segÃºn tu esquema)
    if (successful.length > 0) {
      console.log('\nğŸ’¾ Guardando ofertas en la base de datos...');
      await this.saveJobPostings(successful);
      console.log('âœ… Ofertas guardadas');
    }

    // Paso 5: Generar resumen
    this.generateSummary(successful, failed);
  }

  /**
   * Guarda las ofertas extraÃ­das en la base de datos
   * TODO: Implementar segÃºn tu esquema de base de datos
   */
  private async saveJobPostings(jobs: ExtractJobResponse[]): Promise<void> {
    // Por ahora solo logueamos, deberÃ¡s implementar la lÃ³gica de guardado
    console.log('TODO: Implementar guardado en base de datos');
    console.log(`  - Ofertas a guardar: ${jobs.length}`);
    
    // Ejemplo de estructura que podrÃ­as guardar:
    for (const job of jobs.slice(0, 3)) { // Solo mostrar 3 ejemplos
      if (job.job_data) {
        console.log(`  - ${job.job_data.title} en ${job.job_data.company}`);
      }
    }
  }

  /**
   * Genera un resumen del scraping
   */
  private generateSummary(successful: ExtractJobResponse[], failed: ExtractJobResponse[]): void {
    console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('ğŸ“Š RESUMEN DE SCRAPING');
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log(`Fuentes procesadas: ${JOB_SOURCES.length}`);
    console.log(`Ofertas exitosas: ${successful.length}`);
    console.log(`Ofertas fallidas: ${failed.length}`);
    console.log(`Tasa de Ã©xito: ${((successful.length / (successful.length + failed.length)) * 100).toFixed(1)}%`);
    
    if (successful.length > 0) {
      console.log('\nâœ… Primeras 5 ofertas exitosas:');
      successful.slice(0, 5).forEach((job, idx) => {
        if (job.job_data) {
          console.log(`  ${idx + 1}. ${job.job_data.title || 'Sin tÃ­tulo'} - ${job.job_data.company || 'Sin empresa'}`);
        }
      });
    }
    
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  }

  /**
   * Detiene todos los cron jobs
   */
  stop() {
    console.log('ğŸ›‘ Deteniendo cron jobs...');
    this.jobs.forEach((task, name) => {
      task.stop();
      console.log(`  - Detenido: ${name}`);
    });
    this.jobs.clear();
  }

  /**
   * Ejecuta manualmente el job de scraping (para testing)
   */
  async runScrapingNow(): Promise<void> {
    console.log('ğŸš€ Ejecutando scraping manual...');
    await this.scrapeJobPostings();
  }

  /**
   * Obtiene el estado de todos los jobs
   */
  getStatus(): { name: string; running: boolean }[] {
    return Array.from(this.jobs.entries()).map(([name, task]) => ({
      name,
      running: task.getStatus() === 'scheduled'
    }));
  }
}

// Exportar instancia singleton
export const cronJobService = new CronJobService();
